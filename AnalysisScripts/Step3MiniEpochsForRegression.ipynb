{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /System/Volumes/Data/misc/data12/sjapee/Sebastian-OrientationImagery/Data/Bids/derivatives/preprocessed/sub-S01_Dynamic_preprocessed-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...    3200.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Reading /System/Volumes/Data/misc/data12/sjapee/Sebastian-OrientationImagery/Data/Bids/derivatives/preprocessed/sub-S01_Dynamic_preprocessed-epo-1.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...    3200.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 8 columns\n",
      "504 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "import mne \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# parameters\n",
    "n_samples_per_miniepoch = 60\n",
    "n_samples_ini = 20\n",
    "n_miniepochs_per_trial = 60\n",
    "\n",
    "\n",
    "# load data\n",
    "bids_dir = '/System/Volumes/Data/misc/data12/sjapee/Sebastian-OrientationImagery/Data/Bids'\n",
    "subject = 'S01'\n",
    "fn_movie = f'sub-{subject}_Dynamic_preprocessed-epo.fif'\n",
    "epochs_movie = mne.read_epochs(f'{bids_dir}/derivatives/preprocessed/{fn_movie}')\n",
    "epochs_movie=epochs_movie[epochs_movie.metadata['condition']!='catch']\n",
    "\n",
    "# initialize\n",
    "n_trials = epochs_movie._data.shape[0]\n",
    "n_channels = epochs_movie._data.shape[1]\n",
    "n_timepoints = epochs_movie._data.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 6 columns\n",
      "Adding metadata with 6 columns\n",
      "26880 matching events found\n",
      "No baseline correction applied\n",
      "Overwriting existing file.\n"
     ]
    }
   ],
   "source": [
    "onset_first_miniepoch = np.where(epochs_movie.times==0)[0][0]+n_samples_ini\n",
    "\n",
    "# setup for metadata\n",
    "miniepochs = []\n",
    "condition = epochs_movie.metadata['condition']\n",
    "direction = [c.split('_')[1] for c in condition]\n",
    "right_idx = [d=='Right' for d in direction]\n",
    "left_idx = [d=='Left' for d in direction]\n",
    "degree = [int(c.split('_')[0]) for c in condition] \n",
    "run = epochs_movie.metadata['run_nr']\n",
    "\n",
    "for i in range(n_miniepochs_per_trial):\n",
    "    # onset shifts by 20 samples, going over the entire movie\n",
    "    # remember the first mini epoch is not at image onset but 20 samples later\n",
    "    onset = onset_first_miniepoch+i*n_samples_per_miniepoch\n",
    "    me = mne.EpochsArray(epochs_movie._data[:,:,onset:onset+n_samples_per_miniepoch],info=epochs_movie.info)\n",
    "\n",
    "    # work out the wrap-around in circular space (can't have numbers >360 or <0)\n",
    "    deg = np.zeros(len(degree))\n",
    "    deg[right_idx] = np.array(degree)[right_idx]+3*i\n",
    "    deg[left_idx] = np.array(degree)[left_idx]-3*i\n",
    "\n",
    "    deg[deg>359] = deg[deg>359]-360\n",
    "    deg[deg<0] = 359-np.abs(deg[deg<0])\n",
    "\n",
    "    # make pandas metadata\n",
    "    df = pd.DataFrame()\n",
    "    df['condition']=condition\n",
    "    df['direction']=direction\n",
    "    df['degrees']=deg\n",
    "    df['timepoints']=i\n",
    "    df['trialnum']=np.arange(n_trials)\n",
    "    df['run'] = run\n",
    "\n",
    "    me.metadata = df\n",
    "    miniepochs.append(me)\n",
    "\n",
    "# concatenate all timepoints\n",
    "mini_epochs = mne.concatenate_epochs(miniepochs)\n",
    "\n",
    "# save them\n",
    "file_name = f'{bids_dir}/derivatives/preprocessed/sub-{subject}_miniEpochs_preprocessed-epo.fif'\n",
    "mini_epochs.save(file_name, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(mini_epochs._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pylab\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stim_duration = 3\n",
    "run_length = 4\n",
    "first_run_file = 1\n",
    "last_run_file = 4\n",
    "S = 'S01'\n",
    "bids_top_dir = '/System/Volumes/Data/misc/data12/sjapee/Sebastian-OrientationImagery/Data/Bids/'\n",
    "#proj_path = \"/misc/data12/sjapee/Sebastian-OrientationImagery/Data/\"\n",
    "#raw_dir = proj_path + '20240611/' + S + '/'\n",
    "#logfile_dir = raw_dir + 'sheets/extracted_dataMovie'\n",
    "bids_dir = bids_top_dir + f'bids_dir/sub-{S}/ses-1/meg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************#\n",
    "### PARAMETERS ###\n",
    "#*****************************#\n",
    "\n",
    "l_freq                      = 0.1\n",
    "h_freq                      = 100\n",
    "notch                       = 60\n",
    "notch_max                   = 240\n",
    "pre_stim_time               = -.2\n",
    "post_stim_time              = 3.2\n",
    "std_deviations_above_below  = 4\n",
    "output_resolution           = 200\n",
    "trigger_channel             = 'UPPT001'\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "dsets = []\n",
    "for i in range(1, 5, 1):\n",
    "    ds = f'run-{i:02}'  # Format the run number with leading zeros\n",
    "    pattern = f'{bids_dir}*Dynamic_{ds}_meg.ds'  # Adjust the pattern to match your file name structure\n",
    "    matches = glob.glob(pattern)\n",
    "    \n",
    "    if matches:\n",
    "        dsets.append(matches[0])\n",
    "    else:\n",
    "        print(f\"No files found for pattern: {pattern}\")\n",
    "\n",
    "print(dsets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way of defining onsets not based on standard deviation but based on finding photodiode after the trigger\n",
    "def defineOnsets(raw, events):\n",
    "    log_statements = []\n",
    "    pd_dat = raw[raw.ch_names.index('UADC016-2104')][0][0]\n",
    "    if np.mean(np.abs(pd_dat))>0.6: # this should be fine but may not be good if there are a looot of zeros\n",
    "        # zero-centering & making everything positive\n",
    "        pd_dat = np.abs(pd_dat-np.mean(pd_dat[0:100]))\n",
    "        # scaling channel to 0-1\n",
    "        pd_dat = (pd_dat-np.min(pd_dat)) / (np.max(pd_dat)-np.min(pd_dat))\n",
    "\n",
    "        print(pd_dat)\n",
    "\n",
    "        # using the event, we are now looking in the next 50ms of the photodiode channel (digital) when the signal goes above 0.5\n",
    "        t_samples = int(0.05/(1/raw.info['sfreq']))\n",
    "        threshold =  .5\n",
    "\n",
    "        # when we found the events we calculate the trigger delay. \n",
    "        pd_events = [events[i,0]+np.where(pd_dat[events[i,0]:events[i,0]+t_samples]>threshold)[0][0] for i in range(len(events))]\n",
    "        pd_onsets = []\n",
    "        pd_offsets = []\n",
    "\n",
    "        for event in events:\n",
    "            # Find onset\n",
    "            onset_index = event[0] + np.argmax(pd_dat[event[0]:event[0] + t_samples] > threshold)\n",
    "            pd_onsets.append(onset_index)\n",
    "\n",
    "            # Find offset\n",
    "            pd_offset_index = onset_index + np.argmax(pd_dat[onset_index:] < threshold)\n",
    "            pd_offsets.append(pd_offset_index)\n",
    "        trigger_delay = 1000. * (pd_events-events[:, 0]) / raw.info['sfreq']\n",
    "        # sometimes this channel records ON as UP and sometimes as DOWN. This if-loop makes sure we always find the onsets\n",
    "        if np.mean(trigger_delay)==0:\n",
    "            pd_events = [events[i,0]+np.where(pd_dat[events[i,0]:events[i,0]+t_samples]>threshold)[0][0] for i in range(len(events))]\n",
    "            trigger_delay = 1000. * (pd_events-events[:, 0]) / raw.info['sfreq']\n",
    "        log_statements.append(('Trigger delay removed (μ ± σ): %0.1f ± %0.1f ms')\n",
    "            % (np.mean(trigger_delay), np.std(trigger_delay)))\n",
    "        if np.mean(trigger_delay)>10:\n",
    "            events[:,0] = events[:,0]+5\n",
    "            log_statements.append(f'photo diode and parallel port triggers are too far apart. using parallel port trigger ')\n",
    "        else:\n",
    "            log_statements.append(f'photo diode used successfully ')\n",
    "            events[:, 0] = pd_events\n",
    "    else: \n",
    "        # if the pd channel didn't work, add 5 samples as a standard delay\n",
    "        events[:,0] = events[:,0]+5\n",
    "        log_statements.append(f'photo diode did not record anything. using parallel port trigger ')\n",
    "\n",
    "    return raw,events,log_statements, pd_onsets, pd_offsets\n",
    "\n",
    "def notch_filter(raw):\n",
    "    #apply notch filter based on global parameters set at the top\n",
    "    notches = np.arange(notch, notch_max+1, notch)\n",
    "    print(notches)\n",
    "    raw.notch_filter(notches, phase='zero-double', fir_design='firwin2')\n",
    "    return raw\n",
    "def prepMarkers():\n",
    "    subs   = ['S01']\n",
    "    block_conditions = {\n",
    "        'Watch_Still': ['0022_Left', '0022_Right', '0067_Left', '0067_Right', '0112_Left', '0112_Right', \n",
    "                        '0157_Left', '0157_Right', '0202_Left', '0202_Right', '0247_Left', '0247_Right',\n",
    "                        '0292_Left', '0292_Right', '0337_Left', '0337_Right']}\n",
    "    trial_data = []\n",
    "    blocks = list(block_conditions.keys())\n",
    "    for block in blocks:\n",
    "        conditions = block_conditions[block]\n",
    "        for condition in conditions:\n",
    "            block_data = {'Block': block, 'Condition': condition}\n",
    "            trial_data.append(block_data)\n",
    "\n",
    "    df = pd.DataFrame(trial_data)\n",
    "    print(df)\n",
    "    df['Code'] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    ids = df['Code']\n",
    "    conds = df['Block']\n",
    "    orients = df['Condition']\n",
    "    event_id = {}\n",
    "    for i, id in enumerate(ids):\n",
    "        event_id[id] = f'{subs[0]}/{conds[i]}/{orients[i]}'\n",
    "    #keys = [f'{sub}/{cond}/{orient}/{id}' for sub in subs for id in ids for cond in conds for orient in orients]\n",
    "    return event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_code(code):\n",
    "    if code in {1, 2}:\n",
    "        return 22\n",
    "    elif code in {3, 4}:\n",
    "        return 67\n",
    "    elif code in {5, 6}:\n",
    "        return 112\n",
    "    elif code in {7, 8}:\n",
    "        return 157\n",
    "    elif code in {9, 10}:\n",
    "        return 202\n",
    "    elif code in {11, 12}:\n",
    "          return 247\n",
    "    elif code in {13, 14}:\n",
    "          return 292\n",
    "    elif code in {15, 16}:\n",
    "          return 337\n",
    "\n",
    "def add_additional_events(events):\n",
    "    new_events_list = []\n",
    "    for event in events:\n",
    "        original_code = event[2]\n",
    "        transformed_code = transform_code(original_code)\n",
    "        new_events_list.append([event[0], 0, transformed_code])  # Add the original event with the transformed code\n",
    "        \n",
    "        onset = event[0]\n",
    "        print('onset', onset)\n",
    "        current_code = transformed_code\n",
    "        increment = original_code % 2 == 0  # True if original_code is odd, False if even\n",
    "        \n",
    "        for i in range(1, 60):\n",
    "            new_onset = onset + 20 + (i * 60)\n",
    "            if increment:\n",
    "                current_code = (current_code + 3) % 360\n",
    "            else:\n",
    "                current_code = (current_code - 3) % 360\n",
    "                if current_code < 0:\n",
    "                    current_code = 359\n",
    "            new_events_list.append([new_onset, 0, current_code])\n",
    "\n",
    "    print(new_events_list)\n",
    "            \n",
    "    return np.array(new_events_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.preprocessing import ICA\n",
    "\n",
    "\n",
    "all_epochs = []\n",
    "all_events = []\n",
    "all_onsets = []\n",
    "all_offsets = []\n",
    "initial_events = []\n",
    "for i in range(len(dsets)):\n",
    "    raw = mne.io.read_raw_ctf(dsets[i],system_clock='ignore',preload=True)\n",
    "    events = mne.find_events(raw, stim_channel='UPPT001',min_duration = 0.002)\n",
    "    #print('events', events)\n",
    "    #print('events2', events[-1])\n",
    "    events = events[events[:, 2] != 17]\n",
    "    raw, events, log_statements, onsets, offsets = defineOnsets(raw, events)\n",
    "    #raw, annotations = writeMarkers(event_ids, raw, events, stim_duration, dsets[i])\n",
    "    print('onset list', onsets)\n",
    "    initial_events.append(events)\n",
    "    events = add_additional_events(events)\n",
    "    # remove trailing zeros\n",
    "    endtime = raw.times[events[-1,0]]+10\n",
    "    raw.crop(0,endtime)\n",
    "    raw = notch_filter(raw)\n",
    "    \n",
    "    #ICA thing \n",
    "    epochs = mne.Epochs(raw, events, event_id=None,tmin=0, tmax=1/60, preload=True, baseline=None, picks = 'mag')\n",
    "    print(\"Number of events:\", len(events))\n",
    "    print(\"Number of epochs:\", len(epochs))\n",
    "    print(np.nonzero(list(map(len, epochs.drop_log)))[0])\n",
    "    [n for n, dl in enumerate(epochs.drop_log) if len(dl)]\n",
    "    all_epochs.append(epochs)\n",
    "    all_events.append(events)\n",
    "    all_onsets.append(onsets)\n",
    "    all_offsets.append(offsets)\n",
    "\n",
    "print(len(all_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_events[3][50:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs._data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_head_t_ref = all_epochs[0].info['dev_head_t']\n",
    "\n",
    "for i in range(0, 4):\n",
    "    all_epochs[i].info['dev_head_t'] = dev_head_t_ref\n",
    "\n",
    "epochs_stacked = mne.concatenate_epochs(all_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(62):\n",
    "    print('time', events[i, 0])\n",
    "    print(events[i, 2])\n",
    "print(epochs_stacked.events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "events = epochs_stacked.events\n",
    "\n",
    "# Initialize lists to store metadata\n",
    "condition_list = []\n",
    "frame_list = []\n",
    "timepoint_list = []\n",
    "\n",
    "# Iterate through events to create metadata\n",
    "for i in range(len(events)):\n",
    "    code = events[i, 2]  # Get the event code from the third column\n",
    "\n",
    "    # Determine condition based on grouping every 60 entries\n",
    "    if i % 60 == 0:\n",
    "\n",
    "        condition = f'condition_{code}'  # Use the code of the first entry in the grouping\n",
    "    else:\n",
    "        condition = condition_list[-1]  # Use the last condition label if not a new condition\n",
    "    \n",
    "    # Frame is the event code itself\n",
    "    frame = code\n",
    "    \n",
    "    \n",
    "    # Append to lists\n",
    "    condition_list.append(condition)\n",
    "    frame_list.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_events[0][50:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "events = epochs_stacked.events\n",
    "\n",
    "# Initialize lists to store metadata\n",
    "condition_list = []\n",
    "frame_list = []\n",
    "direction_list = []\n",
    "\n",
    "# Iterate through events to create metadata\n",
    "for i in range(len(events)):\n",
    "    code = events[i, 2]  # Get the event code from the third column\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if i % 60 == 0:\n",
    "        condition = f'condition_{code}'  # Use the code of the first entry in the grouping\n",
    "    else:\n",
    "        condition = condition_list[-1]  # Use the last condition label if not a new condition\n",
    "    \n",
    "    # Frame is the event code itself\n",
    "    frame = code\n",
    "    \n",
    "    \n",
    "    # Append to lists\n",
    "    condition_list.append(condition)\n",
    "    frame_list.append(frame)\n",
    "\n",
    "# Create metadata dictionary\n",
    "metadata = {\n",
    "    'condition': condition_list,\n",
    "    'frame': frame_list,\n",
    "}\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_stacked.metadata = metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs_stacked.metadata[60:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mag = raw.pick_types('mag', ref_meg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_events = np.array(initial_events)\n",
    "\n",
    "# Data dimensions\n",
    "num_events = initial_events[0].shape[0]*4\n",
    "print(num_events)\n",
    "num_sensors = raw_mag._data.shape[0]\n",
    "# Calculate the indices for 240 samples before each event\n",
    "indices_240_before = np.round(initial_events[:, 1] - 240).astype(int)\n",
    "\n",
    "first_elements_master = []\n",
    "for i in initial_events:\n",
    "\n",
    "    first_dimension_list = [row.tolist() for row in i]\n",
    "# Initialize an array to store the averaged data\n",
    "    first_elements = [sublist[0] for sublist in first_dimension_list]\n",
    "    first_elements_master.extend(first_elements)\n",
    "events_minus_240 = [x - 240 for x in first_elements_master]\n",
    "print(first_elements_master)\n",
    "print(events_minus_240)\n",
    "\n",
    "averaged_data = np.zeros((num_events, num_sensors))\n",
    "\n",
    "# Loop through each index directly\n",
    "for i, event_index in enumerate(events_minus_240):\n",
    "    start_index = event_index\n",
    "    end_index = event_index + 240\n",
    "    if end_index > raw_mag._data.shape[1]:\n",
    "        end_index = raw_mag._data.shape[1]\n",
    "    \n",
    "    # Extract the data and average it across the specified window\n",
    "    data_window = raw_mag._data[:, start_index:end_index]\n",
    "    averaged_data[i, :] = np.mean(data_window, axis=1)\n",
    "\n",
    "# averaged_data now contains the averaged data for each event\n",
    "\n",
    "\n",
    "print(averaged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_array = np.repeat(averaged_data, 60, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs_stacked._data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs_stacked._data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_final = epochs_stacked._data - expanded_array[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_stacked._data = epochs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs_stacked._data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'miniEpochsnobaselineTest1-epo.fif'\n",
    "epochs_stacked.save(file_name, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
